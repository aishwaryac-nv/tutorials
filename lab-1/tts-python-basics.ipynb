{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How do I use Riva TTS APIs with out-of-the-box models?\n",
    "\n",
    "This tutorial walks you through the basics of Riva Speech Skills's TTS Services, specifically covering how to use Riva TTS APIs with out-of-the-box models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NVIDIA Riva Overview\n",
    "\n",
    "NVIDIA Riva is a GPU-accelerated SDK for building Speech AI applications that are customized for your use case and deliver real-time performance. <br/>\n",
    "Riva offers a rich set of speech services such as:\n",
    "\n",
    "- Automated speech recognition (ASR)\n",
    "- Text-to-Speech synthesis (TTS)\n",
    "\n",
    "In this tutorial, we will interact with the text-to-speech synthesis (TTS) APIs.\n",
    "\n",
    "For more information about Riva, please refer to the [Riva developer documentation](https://developer.nvidia.com/riva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech generation with Riva TTS APIs\n",
    "\n",
    "The Riva TTS service is based on a two-stage pipeline: Riva first generates a mel spectrogram using the first model, then generates speech using the second model. This pipeline forms a text-to-speech system that enables you to synthesize natural sounding speech from raw transcripts without any additional information such as patterns or rhythms of speech.\n",
    "\n",
    "Riva provides two state-of-the-art voices (one male and one female) for English, that can easily be deployed with the Riva Quick Start scripts. Riva also supports easy customization of TTS in various ways, to meet your specific needs.\n",
    "\n",
    "Subsequent Riva releases will include added features, including model registration to support multiple languages/voices with the same API. Support for resampling to alternative sampling rates will also be added.\n",
    "\n",
    "Refer to the [Riva TTS documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html) for more information.  \n",
    "\n",
    "Now, let's generate audio using Riva APIs with an OOTB (out-of-the-box) English TTS pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Riva clent libraries\n",
    "\n",
    "We first import some required libraries, including the Riva client libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "\n",
    "import riva.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Riva clients and connect to Riva Speech API server\n",
    "\n",
    "The below URI assumes a local deployment of the Riva Speech API server on the default port. In case the server deployment is on a different host or via Helm chart on Kubernetes, the user should use an appropriate URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "\n",
    "riva_tts = riva.client.SpeechSynthesisService(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch mode TTS\n",
    "\n",
    "Riva TTS supports both streaming and batch inference modes. In batch mode, audio is not returned until the full audio sequence for the requested text is generated and can achieve higher throughput. But when making a streaming request, audio chunks are returned as soon as they are generated, significantly reducing the latency (as measured by time to first audio) for large requests. <br> \n",
    "Let's take a look at an example showing batch mode TTS API usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a gRPC request to the Riva Speech API server\n",
    "\n",
    "Now let us make a gRPC request to the Riva Speech servers, for TTS, in batch inference mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Model is not available on server: Voice English-US-Female-1 for language en-US not found. Please specify the voice name in your SynthesizeSpeechRequest.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2023-10-09T16:34:23.284582895+00:00\", grpc_status:3, grpc_message:\"Model is not available on server: Voice English-US-Female-1 for language en-US not found. Please specify the voice name in your SynthesizeSpeechRequest.\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_rate_hz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m44100\u001b[39m\n\u001b[0;32m----> 2\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mriva_tts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIs it recognize speech or wreck a nice beach?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men-US\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mriva\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAudioEncoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLINEAR_PCM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Currently only LINEAR_PCM is supported\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_rate_hz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_rate_hz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Generate 44.1KHz audio\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvoice_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnglish-US-Female-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# The name of the voice to generate\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m audio_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(resp\u001b[38;5;241m.\u001b[39maudio, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint16)\n\u001b[1;32m     11\u001b[0m ipd\u001b[38;5;241m.\u001b[39mAudio(audio_samples, rate\u001b[38;5;241m=\u001b[39msample_rate_hz)\n",
      "File \u001b[0;32m/mnt/nvdl/usr/schilton/nvidia-riva-tutorials/virtualenv-riva-tutorials-py38/lib/python3.8/site-packages/riva/client/tts.py:68\u001b[0m, in \u001b[0;36mSpeechSynthesisService.synthesize\u001b[0;34m(self, text, voice_name, language_code, encoding, sample_rate_hz, future)\u001b[0m\n\u001b[1;32m     66\u001b[0m     req\u001b[38;5;241m.\u001b[39mvoice_name \u001b[38;5;241m=\u001b[39m voice_name\n\u001b[1;32m     67\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mSynthesize\u001b[38;5;241m.\u001b[39mfuture \u001b[38;5;28;01mif\u001b[39;00m future \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub\u001b[38;5;241m.\u001b[39mSynthesize\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_auth_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/nvdl/usr/schilton/nvidia-riva-tutorials/virtualenv-riva-tutorials-py38/lib/python3.8/site-packages/grpc/_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1022\u001b[0m              request: Any,\n\u001b[1;32m   1023\u001b[0m              timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m              wait_for_ready: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1027\u001b[0m              compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1028\u001b[0m     state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m   1029\u001b[0m                                   wait_for_ready, compression)\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/nvdl/usr/schilton/nvidia-riva-tutorials/virtualenv-riva-tutorials-py38/lib/python3.8/site-packages/grpc/_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Model is not available on server: Voice English-US-Female-1 for language en-US not found. Please specify the voice name in your SynthesizeSpeechRequest.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2023-10-09T16:34:23.284582895+00:00\", grpc_status:3, grpc_message:\"Model is not available on server: Voice English-US-Female-1 for language en-US not found. Please specify the voice name in your SynthesizeSpeechRequest.\"}\"\n>"
     ]
    }
   ],
   "source": [
    "sample_rate_hz = 44100\n",
    "resp = riva_tts.synthesize(\n",
    "    text = \"Is it recognize speech or wreck a nice beach?\",\n",
    "    language_code = \"en-US\",\n",
    "    encoding = riva.client.AudioEncoding.LINEAR_PCM,    # Currently only LINEAR_PCM is supported\n",
    "    sample_rate_hz = sample_rate_hz,                    # Generate 44.1KHz audio\n",
    "    voice_name = \"English-US.Female-1\"         # The name of the voice to generate\n",
    ")\n",
    "\n",
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding TTS API parameters\n",
    "\n",
    "Riva TTS supports a number of options while making a text-to-speech request to the gRPC endpoint, as shown above. Let's learn more about these parameters:\n",
    "- ``language_code`` - Language of the generated audio. ``\"en-US\"`` represents English (US) and is currently the only language supported OOTB.\n",
    "- ``encoding`` - Type of audio encoding to generate (``LINEAR_PCM``, ``FLAC``, ``MULAW`` and ``ALAW``).\n",
    "- ``sample_rate_hz`` - Sample rate of the generated audio. Depends on the microphone and is usually ``22khz`` or ``44khz``.\n",
    "- ``voice_name`` - Voice used to synthesize the audio. Currently, Riva offers two OOTB voices (``English-US-Female-1``, ``English-US-Male-1``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we come to an end for the introduction to Riva's offline python client for TTS. Feel free to read more about the Riva's TTS proto [here](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/reference/protos/riva_tts.proto.html?highlight=proto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv-riva-tutorials-py38",
   "language": "python",
   "name": "virtualenv-riva-tutorials-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
