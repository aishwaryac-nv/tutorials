{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_tts_tts-python-advanced-pretrain-tts-tao-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to deploy custom TTS Models (FastPitch and HiFi-GAN) trained with TAO Toolkit on Riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial walks you the through deployment of custom TTS models (FastPitch and HiFiGAN) trained with TAO Toolkit on RIVA for real-time inference.\n",
    "\n",
    "The custom TTS models trained in the notebook, `3_spectrogen-vocoder-tao-training.ipynb`, will be used for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings)\n",
    "for Riva deployment to a target environment.\n",
    "\n",
    "### Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called an RMIR)\n",
    "of an end-to-end pipeline for the supported services within Riva. Let's consider two TTS models.\n",
    "\n",
    "* [FastPitch](https://ngc.nvidia.com/catalog/models/nvidia:tao:speechsynthesis_english_fastpitch) (spectrogram generator)\n",
    "* [HiFi-GAN](https://ngc.nvidia.com/catalog/models/nvidia:tao:speechsynthesis_hifigan) (vocoder)<br>\n",
    "\n",
    "We'll use the customized spectrogram and vocoder models (from the previous notebook) to deploy the Riva TTS pipeline.\n",
    "\n",
    "Let's set the path to the customized spectrogram generator and vocoder models (`.riva`) which will be used when running `riva-build`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: UPDATE MODEL_LOC with `.riva's` ABSOLUTE PATH \n",
    "import os\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.10.0-servicemaker\"\n",
    "\n",
    "# Directory containing the riva folder, which in turn contains the .riva files\n",
    "MODEL_LOC = os.path.abspath(\"tts-models/results\")\n",
    "\n",
    "# Names of the .riva files contained in $MODEL_LOC/riva\n",
    "SPECTRO_GEN_MODEL_NAME = \"FastPitch.riva\"\n",
    "VOCODER_MODEL_NAME = \"HiFiGan.riva\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY = \"tlt_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory within `$MODEL_LOC` to store the `.rmir` file. This is most useful if deploying multiple models. Moreover, the Riva Server start script, `riva_start.sh`, assumes that the `.rmir` files you deploy will be contained in `$MODEL_LOC/rmir` rather than `$MODEL_LOC`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p $MODEL_LOC/rmir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-build <task-name> output-dir-for-rmir/model.rmir:key dir-for-riva/model.riva:key\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "    riva-build speech_synthesis /data/rmir/tts.rmir:$KEY /data/riva/$SPECTRO_GEN_MODEL_NAME:$KEY /data/riva/$VOCODER_MODEL_NAME:$KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and\n",
    "a target model repository directory. It creates an ensemble configuration specifying the pipeline for\n",
    "the execution and finally writes all those assets to the output model repository directory.\n",
    "\n",
    "**Note**: This step might take ~10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "    riva-deploy -f  /data/rmir/tts.rmir:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. First, download the Riva Skills Quick Start resource folder from NGC. \n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing the Riva Skills Quick Start resource folder\n",
    "RIVA_DIR = os.path.abspath(\"riva_quickstart_v2.10.0\")\n",
    "os.environ['RIVA_DIR'] = RIVA_DIR\n",
    "\n",
    "# Downloads the Riva Skills Quick Start resource folder to the current working directory and uncompresses it\n",
    "if os.path.exists(RIVA_DIR):\n",
    "    print(\"Riva Skills Quick Start resource folder exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Riva Skills Quick Start resource folder\")\n",
    "    !ngc registry resource download-version \"nvidia/riva/riva_quickstart:2.10.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify the `config.sh` file to enable relevant Riva services (TTS for the FastPitch/HiFi-GAN models), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. \n",
    "\n",
    "For example, if above the model repository is generated at `$MODEL_LOC/models`, then we will specify `riva_model_loc` as the same directory as `MODEL_LOC`. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### config.sh snippet\n",
    "```sh\n",
    "# Enable or Disable Riva Services\n",
    "service_enabled_asr=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_nlp=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_tts=true\n",
    "service_enabled_nmt=true          ## MAKE CHANGES HERE - SET TO FALSE\n",
    "\n",
    "...\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified.\n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "#\n",
    "# Custom models produced by NeMo or TLT and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"riva-model-repo\"  ## MAKE CHANGES HERE (Replace with the path TTS_MODEL_DIR)\n",
    "\n",
    "if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n",
    "    riva_model_loc=\"`pwd`/model_repository\"\n",
    "fi\n",
    "\n",
    "# The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "# If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "# then set the below $use_existing_rmirs flag to true. You can also deploy your set of custom\n",
    "# RMIRs by keeping them in the riva_rmir_loc dir and use this quickstart script with the\n",
    "# below flag to deploy them all together.\n",
    "use_existing_rmirs=false          ## MAKE CHANGES HERE - SET TO TRUE                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**ATTENTION:**</font> **Make sure to do the following before moving forward:**\n",
    "\n",
    "**Either** carry out these tasks manually: \n",
    "1. In the file navigator in Jupyter Lab, navigate to `riva_quickstart_v2.*` and open `config.sh`\n",
    "2. Configure settings as shown in the snippet above\n",
    "   - Set ASR, NLP, and NMT services to `false`\n",
    "   - Set the `riva_model_loc` path to the path also assigned to `TTS_MODEL_DIR`\n",
    "   - Set the variable `use_existing_rmirs` to `true`\n",
    "\n",
    "**Or** run the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENABLE_ASR = 'false'\n",
    "ENABLE_NLP = 'false'\n",
    "ENABLE_TTS = 'true'\n",
    "ENABLE_NMT = 'false'\n",
    "\n",
    "!sed -i \"s|service_enabled_asr=.*|service_enabled_asr=$ENABLE_ASR|g\" $RIVA_DIR/config.sh\n",
    "!sed -i \"s|service_enabled_nlp=.*|service_enabled_nlp=$ENABLE_NLP|g\" $RIVA_DIR/config.sh\n",
    "!sed -i \"s|service_enabled_tts=.*|service_enabled_tts=$ENABLE_TTS|g\" $RIVA_DIR/config.sh\n",
    "!sed -i \"s|service_enabled_nmt=.*|service_enabled_nmt=$ENABLE_NMT|g\" $RIVA_DIR/config.sh\n",
    "\n",
    "!sed -i \"/\\sriva_model_loc=.*/! s|riva_model_loc=.*|riva_model_loc=\\\"$MODEL_LOC\\\"|g\" $RIVA_DIR/config.sh\n",
    "\n",
    "!sed -i \"s|use_existing_rmirs=.*|use_existing_rmirs=true|g\" $RIVA_DIR/config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `riva-model-loc` to where the models resulting from riva-deploy are stored. In our case it is MODEL_LOC\n",
    "!echo $MODEL_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x ./riva_stop.sh && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop existing Riva deployments. \n",
    "! cd $RIVA_DIR && ./riva_stop.sh config.sh \n",
    "# Run Riva Start. This will deploy your model(s).\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up-and-running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, we will use the Riva Python API bindings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Inference\n",
    "Now we can actually query the Riva server. The following cell queries the Riva server (using gRPC) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install nvidia-riva-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import riva.client\n",
    "import IPython.display as ipd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = riva.client.Auth(uri=\"localhost:50051\")\n",
    "riva_tts = riva.client.SpeechSynthesisService(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_hz = 22050\n",
    "resp = riva_tts.synthesize(\n",
    "    text = \"Is it recognize speech or wreck a nice beach?\",\n",
    "    language_code = \"en-US\",\n",
    "    encoding = riva.client.AudioEncoding.LINEAR_PCM,    # Currently only LINEAR_PCM is supported\n",
    "    sample_rate_hz = sample_rate_hz,                    # Generate 22.05 KHz audio\n",
    "    voice_name = None         # The name of the voice to generate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_samples = np.frombuffer(resp.audio, dtype=np.int16)\n",
    "ipd.Audio(audio_samples, rate=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthesized speech might sound robotic because we've trained both the FastPitch and HiFiGAN models for a less number of epochs/iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the Riva ServiceMaker container (and thus shut down the Riva server) before shutting down the Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker container stop riva-speech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
