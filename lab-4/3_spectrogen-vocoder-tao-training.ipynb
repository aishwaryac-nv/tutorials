{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_tts_tts-python-advanced-pretrain-tts-tao-training/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to train Riva TTS models (FastPitch and HiFiGAN) with NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeMo Toolkit is a Python-based AI toolkit for training and customizing purpose-built pre-trained AI models with your own data. In this tutorial, we will train the models from scratch, but one can easily customize them via transfer learning instead. \n",
    "\n",
    "Transfer learning extracts learned features from an existing neural network to a new one. Transfer learning is often used when creating a large training dataset is not feasible.\n",
    "\n",
    "Developers, researchers and software partners building intelligent AI apps and services, can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this in action with a use case for Speech Synthesis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we will customize the Riva TTS pipeline by training Riva TTS models with NVIDIA's NeMo Toolkit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective is to synthesize reasonable and natural speech for given text. Since there are no universal standards to measure the quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained.\n",
    "\n",
    "TTS consists of two models: [FastPitch](https://arxiv.org/pdf/2006.06873.pdf) and [HiFi-GAN](https://arxiv.org/pdf/2010.05646.pdf).\n",
    "\n",
    "* FastPitch is spectrogram model that generates a Mel spectrogram from text input. It's a fully-parallel text-to-speech model based on FastSpeech, conditioned on fundamental frequency contours. The model predicts pitch contours during inference, and generates speech that could be further controlled with predicted contours. FastPitch can thus change the perceived emotional state of the speaker or put emphasis on certain lexical units\n",
    "\n",
    "![FastPitch](./imgs/architecture-fastpitch.PNG)\n",
    "\n",
    "\n",
    "* HiFiGAN is a vocoder model that generates an audio output from the Mel spectrograms generated using FastPitch. HiFiGAN uses an end-to-end feed-forward WaveNet architecture, trained with multi-scale adversarial discriminators in both the time domain and the time-frequency domain. It relies on the deep feature matching losses of the discriminators to improve the perceptual quality of enhanced speech. The proposed model generalizes well to new speakers, new speech content, and new environments. It significantly outperforms state-of-the-art baseline methods in both objective and subjective experiments. \n",
    "\n",
    "![HiFiGAN](./imgs/architecture-hifigan.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Dig in: TTS using NeMo\n",
    "\n",
    "This notebook assumes that you are already familiar with TTS Training using NeMo, as described in the [text-to-speech-training](https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/FastPitch_MixerTTS_Training.ipynb) notebook, and that you have a pretrained TTS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After [installing NeMo](https://github.com/NVIDIA/NeMo#installation), the next step is to setup the paths to save data and results. NeMo can be used with docker containers or virtual environments.\n",
    "\n",
    "Replace the variables FIXME with the required paths enclosed in \"\" as a string.\n",
    "\n",
    "`IMPORTANT NOTE:` Here, we map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container. Make sure this tutorial is in the NeMo folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Installation of packages and importing of files\n",
    "\n",
    "We will first install all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install numba>=0.53\n",
    "! pip install librosa\n",
    "! pip install soundfile\n",
    "! pip install tqdm\n",
    "! pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone NeMo locally\n",
    "# Change this path if you don't want to clone NeMo to the directory containing this tutorial\n",
    "NEMO_DIR = os.path.join(os.getcwd(), \"NeMo\")\n",
    "! git clone https://github.com/NVIDIA/NeMo $NEMO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Install NeMo\n",
    "BRANCH = 'main'\n",
    "! python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"tts-models\"), \"datasets\")\n",
    "RESULTS_DIR = os.path.join(os.path.abspath(\"tts-models\"), \"results\")\n",
    "\n",
    "! mkdir -p {DATA_DIR}\n",
    "! mkdir -p {RESULTS_DIR}\n",
    "\n",
    "os.environ[\"DATA_DIR\"] = DATA_DIR\n",
    "os.environ[\"RESULTS_DIR\"] = RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "In this tutorial, we will illustrate the process of training FastPitch and HiFiGAN from scratch on the LJSpeech dataset. First, let's download and pre-process the original LJSpeech dataset and set variables that point to the associated manifest `.json` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step downloads audio to text file lists from NVIDIA for LJSpeech and generates the manifest files `train_manifest.json`, `val_manifest.json`, and `test_manifest.json`. \n",
    "\n",
    "If you use your own dataset, you have to generate three files: `ljs_audio_text_train_manifest.json`, `ljs_audio_text_val_manifest.json`, `ljs_audio_text_test_manifest.json` yourself. Those files correspond to your train / val / test split. For each text file, the number of rows should be equal to number of samples in this split and each row for a single speaker dataset should be like:\n",
    "\n",
    "```\n",
    "{\"audio_filepath\": \"path_to_audio_file\", \"text\": \"text_of_the_audio\", \"duration\": duration_of_the_audio}\n",
    "```\n",
    "\n",
    "In case of multi-speaker dataset\n",
    "\n",
    "```\n",
    "{\"audio_filepath\": \"path_to_audio_file\", \"text\": \"text_of_the_audio\", \"duration\": duration_of_the_audio, \"speaker\": speaker_id}\n",
    "```\n",
    "\n",
    "An example row is:\n",
    "\n",
    "```\n",
    "{\"audio_filepath\": \"actressinhighlife_01_bowen_0001.flac\", \"text\": \"the pleasant season did my heart employ\", \"duration\": 2.4}\n",
    "```\n",
    "\n",
    "We will now download the audio and the manifest files then convert them to the above format, also normalize the text. These steps for LJSpeech can be found in NeMo [`scripts/dataset_processing/tts/ljspeech/get_data.py`](https://github.com/NVIDIA/NeMo/blob/main/scripts/dataset_processing/tts/ljspeech/get_data.py). Be patient, this step is expected to take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $NEMO_DIR/scripts/dataset_processing/tts/ljspeech/get_data.py \\\n",
    "    --data-root $DATA_DIR \\\n",
    "    --whitelist-path $NEMO_DIR/scripts/dataset_processing/tts/ljspeech/lj_speech.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Pitch Statistics\n",
    "\n",
    "Training Fastpitch requires you to set 2 values for pitch extraction:\n",
    "  - `avg`: The average used to normalize the pitch\n",
    "  - `std`: The std deviation used to normalize the pitch\n",
    "\n",
    "We can compute pitch for the training data using [`scripts/dataset_processing/tts/extract_sup_data.py`](https://github.com/NVIDIA/NeMo/blob/main/scripts/dataset_processing/tts/extract_sup_data.py) and extract pitch statistics using the NeMo script [`scripts/dataset_processing/tts/compute_speaker_stats.py`](https://github.com/NVIDIA/NeMo/blob/main/scripts/dataset_processing/tts/compute_speaker_stats.py), We have already downloaded the files earlier in the tutorial. Let's use it to get `pitch_mean` and `pitch_std`.\n",
    "\n",
    "**Note**: It can take several hours for this script to compute the supplementary statistics for the LJSpeech dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will extract the pitch supplementary data using `extract_sup_data.py` file. This file works with a yaml config file `ds_for_fastpitch_align`, which we downloaded above. To make this work for your dataset simply change the `manifest_path` to your manifest path. The argument `sup_data_path` determines where the supplementary data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the paths to the LJSpeech manifest files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ljspeech_dir = os.path.join(DATA_DIR, \"LJSpeech-1.1\")\n",
    "train_manifest_json = os.path.join(ljspeech_dir, \"train_manifest.json\")\n",
    "val_manifest_json   = os.path.join(ljspeech_dir, \"val_manifest.json\")\n",
    "test_manifest_json  = os.path.join(ljspeech_dir, \"test_manifest.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Path to the directory containing the `ds_for_fastpitch_align.yaml` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(NEMO_DIR, \"scripts/dataset_processing/tts/ljspeech/ds_conf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the output paths for the `extract_sup_data.py` script, or more precisely the `ds_for_fastpitch_align.yaml` configuration file on which it depends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_data_path = os.path.join(ljspeech_dir, \"sup_data_path\")\n",
    "pitch_stats_path = os.path.join(ljspeech_dir, \"pitch_stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to the `extract_sup_data.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_sup_data = os.path.join(NEMO_DIR, \"scripts/dataset_processing/tts/extract_sup_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script [`scripts/dataset_processing/tts/extract_sup_data.py`](https://github.com/NVIDIA/NeMo/blob/main/scripts/dataset_processing/tts/extract_sup_data.py) writes the pitch mean and pitch std (standard deviation) in the command line. We'll route that output to a file, then read and parse the file to extract the pitch mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cd $NEMO_DIR && \\\n",
    "    python $extract_sup_data --config-path=$config_path \\\n",
    "    manifest_filepath=$train_manifest_json sup_data_path=$sup_data_path \\\n",
    "    &> $ljspeech_dir/sup_data_console_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(ljspeech_dir, \"sup_data_console_output.txt\"), \"r\") as f:\n",
    "    cmd_str_list = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd_str = [c for c in cmd_str_list if \"PITCH_MEAN\" in c][0]\n",
    "cmd_str = cmd_str[cmd_str.find('PITCH_MEAN='):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract pitch mean and std from the command line\n",
    "pitch_mean_str = cmd_str.split(',')[0]\n",
    "pitch_mean = float(pitch_mean_str.split('=')[1])\n",
    "pitch_std_str = cmd_str.split(',')[1]\n",
    "pitch_std = float(pitch_std_str.split('=')[1])\n",
    "pitch_mean, pitch_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the `pitch_mean` and `pitch_std` based on the results from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"pitch_mean\"] = str(pitch_mean)\n",
    "os.environ[\"pitch_std\"] = str(pitch_std)\n",
    "\n",
    "print(f\"pitch mean: {pitch_mean}\")\n",
    "print(f\"pitch std: {pitch_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We are now ready to train our TTS models. We'll start with FastPitch, then proceed to HiFiGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training FastPitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [`examples/tts/fastpitch.py`](https://github.com/NVIDIA/NeMo/blob/main/examples/tts/fastpitch.py) to train FastPitch. Doing so properly will take many more epochs than the default value of 10 given here. \n",
    "\n",
    "If you wish to fine-tune FastPitch with your own dataset, use [`examples/tts/fastpitch_finetune.py`](https://github.com/NVIDIA/NeMo/blob/main/examples/tts/fastpitch_finetune.py) instead. Change the dataset arguments accordingly, and add the argument `+init_from_pretrained_model=\"tts_en_fastpitch\"`. This will initialize the model with the pretrained [FastPitch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_fastpitch) checkpoint available from NGC. For more details, refer to this [TTS Fine-Tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/tts-finetune-nemo.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd $NEMO_DIR && \\\n",
    "  python $NEMO_DIR/examples/tts/fastpitch.py \\\n",
    "  --config-name=fastpitch_align_v1.05.yaml \\\n",
    "  train_dataset=$train_manifest_json \\\n",
    "  validation_datasets=$val_manifest_json \\\n",
    "  sup_data_path=$sup_data_path \\\n",
    "  exp_manager.exp_dir=$RESULTS_DIR \\\n",
    "  trainer.max_epochs=10 \\\n",
    "  trainer.check_val_every_n_epoch=10 \\\n",
    "  model.train_ds.dataloader_params.batch_size=24 \\\n",
    "  model.validation_ds.dataloader_params.batch_size=24 \\\n",
    "  model.n_speakers=1 \\\n",
    "  model.pitch_mean=$pitch_mean \\\n",
    "  model.pitch_std=$pitch_std \\\n",
    "  model.optim.lr=2e-4 \\\n",
    "  ~model.optim.sched \\\n",
    "  model.optim.name=adam \\\n",
    "  trainer.devices=1 \\\n",
    "  trainer.strategy=null \\\n",
    "  +model.text_tokenizer.add_blank_at=true \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the training command:\n",
    "\n",
    "* `--config-name=fastpitch_align_v1.05.yaml`\n",
    "  * We first tell the script what config file to use.\n",
    "\n",
    "* `train_dataset=$train_manifest_json \n",
    "  validation_datasets=$val_manifest_json \n",
    "  sup_data_path=$sup_data_path`\n",
    "  * We tell the script what manifest files to train and eval on, as well as where supplementary data is located (or will be calculated and saved during training if not provided).\n",
    "  \n",
    "* `phoneme_dict_path=tts_dataset_files/cmudict-0.7b_nv22.10 \n",
    "heteronyms_path=tts_dataset_files/heteronyms-052722\n",
    "whitelist_path=tts_dataset_files/tts.tsv \n",
    "`\n",
    "  * We tell the script where `phoneme_dict_path`, `heteronyms-052722` and `whitelist_path` are located. These are the additional files we downloaded earlier, and are used in preprocessing the data.\n",
    "  \n",
    "* `trainer.max_epochs=10 trainer.check_val_every_n_epoch=10`\n",
    "  * For this experiment, we tell the script to train for 10 epochs. You will need to train FastPitch for many more epochs to obtain good results.\n",
    "\n",
    "* `model.train_ds.dataloader_params.batch_size=24 model.validation_ds.dataloader_params.batch_size=24`\n",
    "  * Set batch sizes for the training and validation data loaders.\n",
    "\n",
    "* `model.n_speakers=1`\n",
    "  * The number of speakers in the data. There is only 1 for now, but we will revisit this parameter later in the notebook.\n",
    "\n",
    "* `model.pitch_mean=$pitch_mean model.pitch_std=$pitch_std`\n",
    "  * Pitch statistics which we computed by running the script `python <NeMo_base>/scripts/dataset_processing/tts/extract_sup_data.py manifest_filepath=<your_manifest_path>`.\n",
    "  * `model.pitch_fmin` and `model.pitch_fmax` are hyperparameters to librosa's pyin function. We recommend tweaking these only if the speaker is in a noisy environment, such that background noise isn't predicted to be speech.\n",
    "\n",
    "* `model.optim.lr=2e-4 ~model.optim.sched model.optim.name=adam`\n",
    "  * For fine-tuning, we lower the learning rate.\n",
    "  * We use a fixed learning rate of 2e-4.\n",
    "  * We switch from the lamb optimizer to the adam optimizer.\n",
    "\n",
    "* `trainer.devices=1 trainer.strategy=null`\n",
    "  * For this notebook, we default to 1 gpu which means that we do not need ddp.\n",
    "  * If you have the compute resources, feel free to scale this up to the number of free gpus you have available.\n",
    "  * Please remove the `trainer.strategy=null` section if you intend on multi-gpu training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Mel Spectrograms\n",
    "\n",
    "We'll use the [`scripts/dataset_processing/tts/generate_mels.py`](https://github.com/NVIDIA/NeMo/blob/main/scripts/dataset_processing/tts/generate_mels.py) script to pass the training data into FastPitch and generate mel spectrograms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to `RESULTS_DIR`, your FastPitch model checkpoint should have a path of the form `FastPitch/<START DATE & TIME>/checkpoints/FastPitch--val_loss=<val_loss>-epoch=<epoch>.ckpt`. Modify it accordingly in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_checkpoint = os.path.join(RESULTS_DIR, \"FastPitch/<START DATE & TIME>/checkpoints/FastPitch--val_loss=<val_loss>-epoch=<epoch>.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the mel spectrograms from the training data. They'll be placed in a folder named `mels` and catalogued in `train_maniffest_mel.json`, both of which will be contained in `ljspeech_dir`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd $NEMO_DIR && \\\n",
    "  python $NEMO_DIR/scripts/dataset_processing/tts/generate_mels.py \\\n",
    "  --fastpitch-model-ckpt $fastpitch_checkpoint \\\n",
    "  --input-json-manifests $train_manifest_json \\\n",
    "  --output-json-manifest-root $ljspeech_dir \\\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training HiFiGAN\n",
    "\n",
    "Now let's train HiFiGAN with the [examples/tts/hifigan.py](https://github.com/NVIDIA/NeMo/blob/main/examples/tts/hifigan.py) script and the configs present in [examples/tts/conf/hifigan](https://github.com/NVIDIA/NeMo/tree/main/examples/tts/conf/hifigan). Doing so properly will take many more steps than the default value of 10000 given here.\n",
    "\n",
    "If you wish to fine-tune HiFiGAN, use [examples/tts/hifigan_finetune.py](https://github.com/NVIDIA/NeMo/blob/main/examples/tts/hifigan_finetune.py) instead. You'll need to generate mel spectrograms from your fine-tuning data instead of the LJSpeech training data. You should also add the argument `+init_from_pretrained_model=tts_hifigan` in calling the fine-tuning script. This will initialize the model with the pretrained [HiFiGAN](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_hifigan) checkpoint available from NGC. For more details, refer to this [TTS Fine-Tuning tutorial](https://github.com/nvidia-riva/tutorials/blob/main/tts-finetune-nemo.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small validation dataset for HiFiGAN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hifigan_train_ds = os.path.join(ljspeech_dir, \"train_manifest_mel.json\")\n",
    "hifigan_val_ds   = os.path.join(ljspeech_dir, \"val_manifest_mel.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat $hifigan_train_ds | tail -n 2 > $hifigan_val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to train HiFiGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!(cd $NEMO_DIR && \\\n",
    "  python $NEMO_DIR/examples/tts/hifigan.py \\\n",
    "  --config-name=hifigan.yaml \\\n",
    "  model.train_ds.dataloader_params.batch_size=32 \\\n",
    "  model.max_steps=10000 \\\n",
    "  model.optim.lr=0.00001 \\\n",
    "  ~model.optim.sched \\\n",
    "  train_dataset=$hifigan_train_ds \\\n",
    "  validation_datasets=$hifigan_val_ds \\\n",
    "  exp_manager.exp_dir=$RESULTS_DIR \\\n",
    "  trainer.check_val_every_n_epoch=10 \\\n",
    "  model/train_ds=train_ds_finetune \\\n",
    "  model/validation_ds=val_ds_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As aforementioned, since there are no universal standard to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained. Therefore, we do not provide `evaluate` functionality in NeMo Toolkit for TTS but only provide `infer` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate spectrogram and audio\n",
    "\n",
    "The first step for inference is generating spectrogram. That's a numpy array (saved as `.npy` file) for a sentence which can be converted to voice by a vocoder. We use FastPitch we just trained to generate spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please update the `hifigan_checkpoint` variable with the path to the HiFiGAN checkpoint you want to use. Relative to `RESULTS_DIR`, it should have a path of the form `HiFiGan/<START DATE & TIME>/checkpoints/HiFiGan--val_loss=<val_loss>-epoch=<epoch>.ckpt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan_checkpoint = os.path.join(RESULTS_DIR, \"HifiGan/<START DATE & TIME>/checkpoints/HifiGan--val_loss=<val_loss>-epoch=<epoch>.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the two models, FastPitch and HiFiGAN, for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "\n",
    "HOME_DIR = os.getcwd()\n",
    "os.chdir(NEMO_DIR)\n",
    "\n",
    "vocoder = HifiGanModel.load_from_checkpoint(hifigan_checkpoint)\n",
    "vocoder = vocoder.eval().cuda()\n",
    "spec_model = FastPitchModel.load_from_checkpoint(fastpitch_checkpoint)\n",
    "spec_model.eval().cuda()\n",
    "\n",
    "os.chdir(HOME_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper method to run inference given a string input. In case of multi-speaker inference the same method can\n",
    "be used by passing the speaker ID as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def infer(spec_gen_model, vocoder_model, str_input, speaker=None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        speaker: Speaker ID\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker=speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for reading manifest `.json` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def json_reader(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell will generate the following for each line in the manifest `.json` file for your test data: \n",
    "- The ground truth audio sample\n",
    "- A mel spectrogram generated by passing the transcribed text into your trained FastPitch model\n",
    "- A synthesized audio sample generated by passing the mel spectrogram into your trained HiFiGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Path to test manifest file (.json)\n",
    "test_records_path = os.path.join(ljspeech_dir, 'test_manifest.json')\n",
    "test_records = list(json_reader(test_records_path))\n",
    "new_speaker_id = None\n",
    "\n",
    "for test_record in test_records:\n",
    "    print(\"Real validation audio\")\n",
    "    ipd.display(ipd.Audio(test_record['audio_filepath'], rate=22050))\n",
    "    duration_sec = test_record['duration']\n",
    "    if 'speaker' in test_record:\n",
    "        speaker_id = test_record['speaker']\n",
    "    else:\n",
    "        speaker_id = new_speaker_id\n",
    "    print(f\"SYNTHESIZED | Duration: {duration_sec} sec | Text: {test_record['text']}\")\n",
    "    spec, audio = infer(spec_model, vocoder, test_record['text'], speaker=speaker_id)\n",
    "    ipd.display(ipd.Audio(audio, rate=22050))\n",
    "    %matplotlib inline\n",
    "    imshow(spec, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug\n",
    "\n",
    "The data provided is only meant to be a sample to understand how finetuning works in NeMo. In order to generate better speech quality, you will need to train FastPitch for far more than the default `trainer.max_epochs=10` epochs and HiFiGAN for far more than the default `model.max_steps=10000` steps. \n",
    "\n",
    "If you're fine-tuning pre-trained models, we recommend recording at least 30 mins of your own audio, and setting the number of fine-tuning steps for both models to `trainer.max_steps=5000`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS model export\n",
    "\n",
    "You can also export your model in a format that can deployed using NVIDIA Riva, a highly performant application framework for multi-modal conversational AI services using GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to RIVA\n",
    "\n",
    "Executing the snippets in the cells below, allows you to generate a `.riva` model file for the spectrogram generator and vocoder models that were trained the preceding cells. These models are required to generate a complete Text-To-Speech pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Riva\n",
    "\n",
    "Convert the downloaded model to `.riva` format. We will use encryption key `tlt_encode`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't manage to generate a `.nemo` file for either FastPitch or HiFiGAN (for example, if your session timed out before the script reached `trainer.max_epochs` or `model.max_steps`), you can create them from the `spec_model` and `vocoder` local model variables which you specified earlier. In that event, uncomment the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spec_model.save_to(os.path.join(os.path.dirname(fastpitch_checkpoint), 'FastPitch.nemo'))\n",
    "# vocoder.save_to(os.path.join(os.path.dirname(hifigan_checkpoint), 'HiFiGan.nemo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths to your FastPitch and HiFiGAN `.nemo` models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_nemo_file_path = FIXME\n",
    "hifigan_nemo_file_path = FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the corresponding `.riva` file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RIVA_MODEL_DIR = os.path.join(RESULTS_DIR, \"riva\")\n",
    "!mkdir -p $RIVA_MODEL_DIR\n",
    "\n",
    "fastpitch_nemo_file_list = fastpitch_nemo_file_path.split('/')\n",
    "fastpitch_nemo_file_name = fastpitch_nemo_file_list[-1]\n",
    "fastpitch_riva_file_name = fastpitch_nemo_file_name[:-5] + \".riva\"\n",
    "fastpitch_riva_file_path = os.path.join(RIVA_MODEL_DIR, fastpitch_riva_file_name)\n",
    "\n",
    "hifigan_nemo_file_list = hifigan_nemo_file_path.split('/')\n",
    "hifigan_nemo_file_name = hifigan_nemo_file_list[-1]\n",
    "hifigan_riva_file_name = hifigan_nemo_file_name[:-5] + \".riva\"\n",
    "hifigan_riva_file_path = os.path.join(RIVA_MODEL_DIR, hifigan_riva_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `nemo2riva` from the `.whl` file provided in the Riva Skills Quick Start resource folder which you downloaded in the first tutorial in this lab. Alternatively, you can install it from PyPI by running \n",
    "```\n",
    "! pip install nemo2riva\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RIVA_DIR = os.path.abspath('riva_quickstart_v2.10.0')\n",
    "! cd $RIVA_DIR && pip install nemo2riva*.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the `.nemo` files to `.riva`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nemo2riva --out $fastpitch_riva_file_path --key=tlt_encode $fastpitch_nemo_file_path\n",
    "! nemo2riva --out $hifigan_riva_file_path   --key=tlt_encode $hifigan_nemo_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next ?\n",
    "\n",
    "Now that we've trained FastPitch and HiFiGAN, proceed to the [next tutorial](./4_spectrogen-vocoder-tao-deployment.ipynb) in this lab to learn how to deploy these models to NVIDIA Riva."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "741d73fab70d7eb29e7b56260ebaa567f0620f4d2780830ca385f600e5120e14"
  },
  "kernelspec": {
   "display_name": "virtualenv-riva-tutorials-py38",
   "language": "python",
   "name": "virtualenv-riva-tutorials-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
